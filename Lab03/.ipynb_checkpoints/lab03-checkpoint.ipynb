{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PierfrancescoElia/AICyberLabs/blob/main/Lab03/lab03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_AVzWfvLVZt"
      },
      "source": [
        "# Project 3: Natural Language Processing (NLP)\n",
        "\n",
        "| Member | ID |\n",
        "| -- | -- |\n",
        "| Alessandro Meneghini| s332228 |\n",
        "| Pierfrancesco Elia | s331497 |\n",
        "| Ankesh Porwal | s328746 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DHgjB2eNIls"
      },
      "source": [
        "## Task 1: Dataset Characterization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T013p6gaNEEJ"
      },
      "outputs": [],
      "source": [
        "from itertools import chain\n",
        "from copy import deepcopy\n",
        "from collections import Counter,defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib.patches import Patch\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from matplotlib.ticker import ScalarFormatter\n",
        "from tqdm.auto import tqdm\n",
        "import base64\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from datasets import Dataset\n",
        "from datasets import DatasetDict\n",
        "\n",
        "from transformers import AutoModelForTokenClassification\n",
        "from transformers import AutoConfig\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from transformers import get_scheduler\n",
        "from transformers import AutoTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-nMcB68NJzJ"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "train_url = 'https://raw.githubusercontent.com/PierfrancescoElia/AICyberLabs/refs/heads/main/Lab03/train.json'\n",
        "test_url = 'https://raw.githubusercontent.com/PierfrancescoElia/AICyberLabs/refs/heads/main/Lab03/test.json'\n",
        "train_data = pd.read_json(train_url)\n",
        "test_data = pd.read_json(test_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83bG3-Zj3CmQ"
      },
      "outputs": [],
      "source": [
        "display(train_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rguh55ta3CmV"
      },
      "source": [
        "### Exploring the Label Distribution\n",
        "\n",
        "- The dataset contains **7 unique MITRE tactic tags**.\n",
        "- Most common tag: **Discovery** .\n",
        "- Least common tag: **Impact** .\n",
        "- The tag distribution is similar between train and test sets, but the dataset is imbalanced, with some tactics much more frequent than others.\n",
        "\n",
        "*See the barplot below for the full distribution across both splits.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLWDSvnv3CmX"
      },
      "outputs": [],
      "source": [
        "train_labels = []\n",
        "test_labels = []\n",
        "\n",
        "for l in train_data['label']:\n",
        "    train_labels.extend(l)\n",
        "\n",
        "for l in test_data['label']:\n",
        "    test_labels.extend(l)\n",
        "\n",
        "print(f\"Unique tag values are {len(set(train_labels + test_labels))}\")\n",
        "print(set(train_labels + test_labels))\n",
        "print()\n",
        "\n",
        "fig, axs = plt.subplots(1, 1, figsize=(8, 4))\n",
        "\n",
        "train_tag_counts = pd.Series(train_labels).value_counts().sort_index()\n",
        "test_tag_counts = pd.Series(test_labels).value_counts().sort_index()\n",
        "tag_names = sorted(list(set(train_labels + test_labels)))\n",
        "\n",
        "bar_width = 0.35\n",
        "x = np.arange(len(tag_names))\n",
        "\n",
        "axs.bar(x - bar_width/2, [train_tag_counts.get(tag, 0) for tag in tag_names],\n",
        "        width=bar_width, label='Train', color='skyblue', edgecolor='black')\n",
        "axs.bar(x + bar_width/2, [test_tag_counts.get(tag, 0) for tag in tag_names],\n",
        "        width=bar_width, label='Test', color='orange', edgecolor='black')\n",
        "\n",
        "axs.set_xticks(x)\n",
        "axs.set_xticklabels(tag_names, rotation=30, ha='right', fontsize=12)\n",
        "\n",
        "axs.set_title(\"Distribution of MITRE Tactic Tags\", fontsize=18, fontweight='bold')\n",
        "axs.set_ylabel(\"Word Count\", fontsize=14)\n",
        "\n",
        "axs.legend(fontsize=18, frameon=False)\n",
        "axs.spines['top'].set_visible(False)\n",
        "axs.spines['right'].set_visible(False)\n",
        "\n",
        "axs.spines['left'].set_linewidth(1.5)\n",
        "axs.spines['bottom'].set_linewidth(1.5)\n",
        "plt.grid(axis='y', linestyle='--')\n",
        "plt.yticks(fontsize=18)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeGRd7Fw3CmY"
      },
      "source": [
        "### Analysis of the `echo` Command Labels\n",
        "\n",
        "- **Number of different tags assigned to `echo`:** 6  \n",
        "- **Occurrences:**  \n",
        "  - Persistence: 104  \n",
        "  - Execution: 39  \n",
        "  - Discovery: 31  \n",
        "  - Not Malicious Yet: 8  \n",
        "  - Impact: 6  \n",
        "  - Other: 4  \n",
        "\n",
        "*Examples for `Persistence` and `Execution` are shown in the output below.*\n",
        "\n",
        "- The `echo` labeled as **Persistence** is changing the root password, a classic method for maintaining access to a system.\n",
        "- The `echo` labeled as **Execution** is used to pipe a base64-encoded command into bash (`| base64 --decode | bash`), directly executing a new payload.\n",
        "\n",
        "The context of how `echo` is used determines its label.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZNtuibX3CmZ"
      },
      "outputs": [],
      "source": [
        "echo_stats = Counter()\n",
        "\n",
        "for words, labels in zip(train_data['session'].str.split(\" \"), train_data['label']):\n",
        "    for word, label in zip(words, labels):\n",
        "        if word == 'echo':\n",
        "            echo_stats[label] += 1\n",
        "\n",
        "print(\"Echo stats:\", echo_stats)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(echo_stats.keys(), echo_stats.values(), color='skyblue', edgecolor='black')\n",
        "plt.xlabel(\"MITRE Tactic Tag\", fontsize=18)\n",
        "plt.ylabel(\"Count of 'echo'\", fontsize=18)\n",
        "plt.title(\"Distribution of 'echo' Command by Tag\", fontsize=18, fontweight='bold')\n",
        "plt.xticks(rotation=30, ha='right', fontsize=14)\n",
        "plt.yticks(fontsize=18)\n",
        "plt.grid(axis='y', linestyle='--')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "found = 0\n",
        "echo_session = None\n",
        "\n",
        "for words, labels in zip(train_data['session'].str.split(\" \"), train_data['label']):\n",
        "    echo_tags = {label for word, label in zip(words, labels) if word == 'echo'}\n",
        "    if 'Persistence' in echo_tags and 'Execution' in labels:\n",
        "        echo_session = (words, labels)\n",
        "        break\n",
        "\n",
        "print(\"Found echo session with Persistence and Execution tags:\\n\", *[i for i in echo_session[0]])\n",
        "print()\n",
        "\n",
        "print(\"context of echo with Persistence tag:\")\n",
        "for j, (word, label) in enumerate(zip(*echo_session)):\n",
        "    if word == 'echo' and label == 'Persistence':\n",
        "        print(\"\\t\", *[i for i in echo_session[0][j:j+5]])\n",
        "print()\n",
        "\n",
        "print(\"context of echo with Execution tag:\")\n",
        "for j, (word, label) in enumerate(zip(*echo_session)):\n",
        "    if word == 'echo' and label == 'Execution':\n",
        "        print(\"\\t\", *[i for i in echo_session[0][j:j+10]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Niu7nl13Cma"
      },
      "outputs": [],
      "source": [
        "print(\"Base64 encoded bash script extracted from echo commands with Execution tags:\\n\")\n",
        "b64string = 'IyEvYmluL2Jhc2gKY2QgL3RtcAkKcm0gLXJmIC5zc2gKcm0gLXJmIC5tb3VudGZzCnJtIC1yZiAuWDEzLXVuaXgKbWtkaXIgLlgxMy11bml4CmNkIC5YMTMtdW5peAptdiAvdmFyL3RtcC9kb3RhLnRhci5neiBkb3RhLnRhci5negp0YXIgeGYgZG90YS50YXIuZ3oKc2xlZXAgM3MgJiYgY2QgL3RtcC8uWDEzLXVuaXgvLnJzeW5jL2MKbm9odXAgL3RtcC8uWDEzLXVuaXgvLnJzeW5jL2MvdHNtIC10IDE1MCAtUyA2IC1zIDYgLXAgMjIgLVAgMCAtZiAwIC1rIDEgLWwgMSAtaSAwIC90bXAvdXAudHh0IDE3Mi4xNiA+PiAvZGV2L251bGwgMj4xJgpzbGVlcCA4bTsgbm9odXAgL3RtcC8uWDEzLXVuaXgvLnJzeW5jL2MvdHNtIC10IDE1MCAtUyA2IC1zIDYgLXAgMjIgLVAgMCAtZiAwIC1rIDEgLWwgMSAtaSAwIC90bXAvdXAudHh0IDE5Mi4xNjggPj4gL2Rldi9udWxsIDI+MSYKc2xlZXAgMjBtICYmIGNkIC90bXAvLlgxMy11bml4LyAmJiBjYXQgL3RtcC8uWDEzLXVuaXgvLnJzeW5jL2luaXRhbGwgfCBiYXNoIDI+MSYKZXhpdCAw'\n",
        "print(base64.b64decode(b64string).decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jf_VU2Y63Cmb"
      },
      "source": [
        "### Session Length Analysis (ECDF)\n",
        "\n",
        "The ECDF plot below shows the distribution of Bash words per session for both train and test sets.\n",
        "\n",
        "- **Median session length:** 17 Bash words (train), 23.5 Bash words (test)\n",
        "- **Mean session length:** 45.7 (train), 57.4 (test)\n",
        "- **90th percentile:** 147 (train), 166.4 (test)\n",
        "- **Maximum:** 224 Bash words (both train and test)\n",
        "\n",
        "Most sessions contain far fewer than 100 Bash words, but a few sessions are much longer. The distributions are similar between train and test, as shown in the ECDF plot above.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgaVB85A3Cmb"
      },
      "outputs": [],
      "source": [
        "train_lengths = train_data['session'].str.split(\" \").apply(len)\n",
        "test_lengths = test_data['session'].str.split(\" \").apply(len)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "x_train = np.sort(train_lengths)\n",
        "y_train = np.arange(1, len(x_train) + 1) / len(x_train)\n",
        "x_test = np.sort(test_lengths)\n",
        "y_test = np.arange(1, len(x_test) + 1) / len(x_test)\n",
        "plt.plot(x_train, y_train, \".-\", label=\"Train\")\n",
        "plt.plot(x_test, y_test, \".-\", label=\"Test\")\n",
        "\n",
        "plt.xlabel(\"Bash Words per Session\", fontsize=18)\n",
        "plt.ylabel(\"ECDF\", fontsize=18)\n",
        "plt.title(\"ECDF of Session Lengths\", fontsize=18, fontweight='bold')\n",
        "plt.xticks(fontsize=18)\n",
        "plt.yticks(fontsize=18)\n",
        "plt.legend(fontsize=18, frameon=False)\n",
        "plt.grid(axis='both', linestyle='--')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print()\n",
        "print(\"Train session length stats:\")\n",
        "print(\"Median:\", train_lengths.median())\n",
        "print(\"Mean:\", train_lengths.mean())\n",
        "print(\"90th percentile:\", np.percentile(train_lengths, 90))\n",
        "print(\"Max:\", train_lengths.max())\n",
        "print()\n",
        "print(\"Test session length stats:\")\n",
        "print(\"Median:\", test_lengths.median())\n",
        "print(\"Mean:\", test_lengths.mean())\n",
        "print(\"90th percentile:\", np.percentile(test_lengths, 90))\n",
        "print(\"Max:\", test_lengths.max())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC3K9QOI3Cmd"
      },
      "source": [
        "## Task 2: Tokenization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_YDau8OaphH"
      },
      "source": [
        "### Tokenization of SSH Commands\n",
        "\n",
        "UniXcoder generally has a better (lower) ratio, meaning fewer tokens per command on average.\n",
        "\n",
        "- **BERT:**\n",
        "  - Out of 9 commands, 4 (`cat`, `shell`, `echo`, `top`) are recognized as a single token.\n",
        "  - The other 5 are split: `chpasswd` and `crontab` into 3 tokens, `wget`, `busybox`, and `grep` into 2 tokens.\n",
        "\n",
        "- **UniXcoder:**\n",
        "  - 6 commands (`cat`, `shell`, `echo`, `top`, `busybox`, `grep`) are single tokens.\n",
        "  - `chpasswd` is split into 2 tokens.\n",
        "  - `crontab` and `wget` are split into 3 and 2 tokens respectively.\n",
        "\n",
        "BERT and UniXcoder both split compound commands, but UniXcoder recognizes more commands as single tokens (6 out of 9), likely due to its pre-training on code and shell data. BERT only treats 4 out of 9 as a single token.  \n",
        "Commands like `top`, `cat`, and `shell` are common words in the English dictionary, so it is expected that both tokenizers keep them as whole tokens.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyQGXKFN3Cmd"
      },
      "outputs": [],
      "source": [
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "unix_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/unixcoder-base\")\n",
        "\n",
        "commands = ['cat', 'shell', 'echo', 'top', 'chpasswd', 'crontab', 'wget', 'busybox', 'grep']\n",
        "\n",
        "table_data = []\n",
        "for cmd in commands:\n",
        "    bert_toks = bert_tokenizer.tokenize(cmd)\n",
        "    unix_toks = unix_tokenizer.tokenize(cmd)\n",
        "    table_data.append({\n",
        "        'Command': cmd,\n",
        "        'BERT tokens': bert_toks,\n",
        "        'Num BERT tokens': len(bert_toks),\n",
        "        'UniXcoder tokens': unix_toks,\n",
        "        'Num UniXcoder tokens': len(unix_toks)\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(table_data)\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXzXnZieceUQ"
      },
      "source": [
        "### Tokenization Statistics on the Training and Test Corpus\n",
        "\n",
        "- **BERT:**\n",
        "  - Average tokens per session: **176.6**\n",
        "  - Median: **69**\n",
        "  - Maximum: **1887**\n",
        "- **UniXcoder:**\n",
        "  - Average tokens per session: **407.3**\n",
        "  - Median: **60**\n",
        "  - Maximum: **28918**\n",
        "\n",
        "UniXcoder produces more tokens on average and has a much higher maximum. This is because UniXcoder never uses `[UNK]` tokens and splits every unknown or rare word into multiple sub-tokens, especially for very long or complex Bash words.\n",
        "BERT instead sometimes uses a single `[UNK]` token for unknown words, resulting in fewer total tokens in such cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nMCjrC7c92a"
      },
      "outputs": [],
      "source": [
        "def print_token_stats(series, name):\n",
        "    desc = series.describe()\n",
        "    print(f\"{name} Tokenization Statistics:\")\n",
        "    print(f\"  Count: {int(desc['count'])}\")\n",
        "    print(f\"  Mean: {desc['mean']:.1f}\")\n",
        "    print(f\"  Std: {desc['std']:.1f}\")\n",
        "    print(f\"  Min: {int(desc['min'])}\")\n",
        "    print(f\"  25%: {int(desc['25%'])}\")\n",
        "    print(f\"  Median: {int(desc['50%'])}\")\n",
        "    print(f\"  75%: {int(desc['75%'])}\")\n",
        "    print(f\"  Max: {int(desc['max'])}\\n\")\n",
        "\n",
        "train_data['bert_tokenized'] = train_data['session'].apply(lambda x: bert_tokenizer.tokenize(x))\n",
        "train_data['uni_tokenized'] = train_data['session'].apply(lambda x: unix_tokenizer.tokenize(x))\n",
        "test_data['bert_tokenized'] = test_data['session'].apply(lambda x: bert_tokenizer.tokenize(x))\n",
        "test_data['uni_tokenized'] = test_data['session'].apply(lambda x: unix_tokenizer.tokenize(x))\n",
        "\n",
        "bert_lengths = train_data['bert_tokenized'].apply(len)\n",
        "uni_lengths = train_data['uni_tokenized'].apply(len)\n",
        "\n",
        "print_token_stats(bert_lengths, \"BERT\")\n",
        "print_token_stats(uni_lengths, \"UniXcoder\")\n",
        "\n",
        "bert_lengths = train_data['bert_tokenized'].apply(len)\n",
        "uni_lengths = train_data['uni_tokenized'].apply(len)\n",
        "\n",
        "print(\"Top 10 Longest UniXcoder Tokenized Sessions:\")\n",
        "for idx, length in uni_lengths.sort_values(ascending=False).head(10).items():\n",
        "    print(f\"  Index: {idx:3d} | Tokens: {length}\")\n",
        "\n",
        "print(\"\\nTop 10 Longest BERT Tokenized Sessions:\")\n",
        "for idx, length in bert_lengths.sort_values(ascending=False).head(10).items():\n",
        "    print(f\"  Index: {idx:3d} | Tokens: {length}\")\n",
        "\n",
        "print(\"\\nExample session details for the longest UniXcoder tokenized session:\")\n",
        "max_idx = uni_lengths.idxmax()\n",
        "print(f\"Session Index: {max_idx}\")\n",
        "print(f\"Original Bash word count: {len(train_data.loc[max_idx, 'session'].split(' '))}\")\n",
        "print(f\"First 200 characters of session: {train_data.loc[max_idx, 'session'][:200]} ...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8I5CHLKd86P"
      },
      "source": [
        "### Sessions Exceeding Maximum Token Length\n",
        "\n",
        "- **BERT maximum token length:** 512  \n",
        "- **UniXcoder maximum token length:** 1000000000000000019884624838656 (very large value)\n",
        "\n",
        "**Number of sessions exceeding max length:**  \n",
        "- **BERT:** 24 sessions  \n",
        "- **UniXcoder:** 0 sessions\n",
        "\n",
        "BERT truncates 24 sessions due to its maximum sequence length of 512 tokens, while UniXcoder does not truncate any sessions because its context limit is extremely large."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bjlhNHq3Cmg"
      },
      "outputs": [],
      "source": [
        "print(\"Model Maximum Token Lengths:\")\n",
        "print(f\"  BERT:      {bert_tokenizer.model_max_length}\")\n",
        "print(f\"  UniXcoder: {unix_tokenizer.model_max_length}\\n\")\n",
        "\n",
        "bert_truncated = train_data['bert_tokenized'].apply(lambda x: len(x) > bert_tokenizer.model_max_length).sum()\n",
        "unix_truncated = train_data['uni_tokenized'].apply(lambda x: len(x) > unix_tokenizer.model_max_length).sum()\n",
        "\n",
        "print(f\"Sessions exceeding max length:\")\n",
        "print(f\"  BERT:      {bert_truncated}\")\n",
        "print(f\"  UniXcoder: {unix_truncated}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hua7fXwreUMp"
      },
      "source": [
        "### Longest Session Analysis\n",
        "\n",
        "- The session with the highest token count (index 14) contains **134 Bash words**.\n",
        "- **UniXcoder:** 28,918 tokens  \n",
        "- **BERT:** 1,887 tokens\n",
        "\n",
        "Both tokenizers produce a high number of tokens because the session contains long, complex Bash words and possibly encoded payloads, which are split into many sub-tokens.\n",
        "\n",
        "BERT produces fewer tokens because it uses the `[UNK]` token for words it cannot split, replacing the entire unknown word with a single token. UniXcoder does not use `[UNK]`; instead, it splits any unknown or rare word into many sub-tokens, resulting in much higher token counts for complex sessions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcLaxszE3Cmh"
      },
      "outputs": [],
      "source": [
        "session_idx = max_idx\n",
        "\n",
        "bash_words = len(train_data.loc[session_idx, 'session'].split(\" \"))\n",
        "\n",
        "print(f\"Session Index: {session_idx}\")\n",
        "print(f\"Number of Bash words: {bash_words}\")\n",
        "print(f\"\\nFirst 50 UniXcoder tokens: {train_data['uni_tokenized'][session_idx][:40]}\")\n",
        "print(f\"\\nFirst 50 BERT tokens: {train_data['bert_tokenized'][session_idx][:40]}\")\n",
        "print(f\"\\nTotal UniXcoder tokens: {len(train_data['uni_tokenized'][session_idx])}\")\n",
        "print(f\"Total BERT tokens: {len(train_data['bert_tokenized'][session_idx])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOXVxewkfNwT"
      },
      "source": [
        "### Truncating Long Words: Tokenization Efficiency\n",
        "\n",
        "After truncating all words longer than 30 characters:\n",
        "\n",
        "- **BERT:**\n",
        "  - Average tokens per session: **126.4**\n",
        "  - Median: **64**\n",
        "  - Maximum: **918**\n",
        "  - **Average tokens per word:** **3.04**\n",
        "- **UniXcoder:**\n",
        "  - Average tokens per session: **108.5**\n",
        "  - Median: **53**\n",
        "  - Maximum: **822**\n",
        "  - **Average tokens per word:** **2.57**\n",
        "\n",
        "Truncating very long words significantly reduces the token count for both models. UniXcoder achieves a slightly better tokens-to-words ratio after truncation, showing better efficiency for code-like and technical data.\n",
        "\n",
        "The scatter plot above shows the relationship between number of words and tokens, with regression lines for both tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOkH0lmg3Cmi"
      },
      "outputs": [],
      "source": [
        "def truncate_long_words(session, max_length=30):\n",
        "    return ' '.join([w[:max_length] for w in session.split(\" \")])\n",
        "\n",
        "train_data['trunc_session'] = train_data['session'].apply(truncate_long_words)\n",
        "test_data['trunc_session'] = test_data['session'].apply(truncate_long_words)\n",
        "\n",
        "train_data['trunc_bert_tokenized'] = train_data['trunc_session'].apply(bert_tokenizer.tokenize)\n",
        "train_data['trunc_uni_tokenized'] = train_data['trunc_session'].apply(unix_tokenizer.tokenize)\n",
        "\n",
        "test_data['trunc_bert_tokenized'] = test_data['trunc_session'].apply(bert_tokenizer.tokenize)\n",
        "test_data['trunc_uni_tokenized'] = test_data['trunc_session'].apply(unix_tokenizer.tokenize)\n",
        "\n",
        "bert_lengths = train_data['trunc_bert_tokenized'].apply(len)\n",
        "uni_lengths = train_data['trunc_uni_tokenized'].apply(len)\n",
        "\n",
        "print(\"BERT (truncated) token stats:\\n\", bert_lengths.describe())\n",
        "print(\"\\nUniXcoder (truncated) token stats:\\n\", uni_lengths.describe())\n",
        "\n",
        "word_counts = train_data['trunc_session'].apply(lambda x: len(x.split(\" \")))\n",
        "bert_ratio = (bert_lengths / word_counts).mean()\n",
        "uni_ratio = (uni_lengths / word_counts).mean()\n",
        "print(f\"\\nAverage tokens per word (BERT): {bert_ratio:.2f}\")\n",
        "print(f\"Average tokens per word (UniXcoder): {uni_ratio:.2f}\")\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X = word_counts.values.reshape(-1, 1)\n",
        "reg_bert = LinearRegression().fit(X, bert_lengths.values)\n",
        "reg_uni = LinearRegression().fit(X, uni_lengths.values)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X, uni_lengths, label='UniXcoder', alpha=0.5, edgecolor='black')\n",
        "plt.scatter(X, bert_lengths, label='BERT', alpha=0.5, edgecolor='black')\n",
        "plt.plot(X, reg_bert.predict(X), color='blue', linewidth=2, label='BERT Regression')\n",
        "plt.plot(X, reg_uni.predict(X), color='orange', linewidth=2, label='UniXcoder Regression')\n",
        "plt.xlabel(\"Number of Words\", fontsize=18)\n",
        "plt.ylabel(\"Number of Tokens\", fontsize=18)\n",
        "plt.title(\"BERT vs UniXcoder Tokenization (with Linear Regression)\", fontsize=18, fontweight='bold')\n",
        "plt.xticks(fontsize=18)\n",
        "plt.yticks(fontsize=18)\n",
        "plt.legend(fontsize=18, frameon=False)\n",
        "plt.grid(axis='both', linestyle='--')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlJbjyC6lLxW"
      },
      "source": [
        "### Truncated Sessions After Truncating Long Words\n",
        "\n",
        "- **BERT:** 6 sessions still exceed the max token length (512 tokens)\n",
        "- **UniXcoder:** 0 sessions truncated (context limit is extremely large)\n",
        "\n",
        "Truncating very long words nearly eliminates sequence truncation issues for both tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4upi-KL93Cmk"
      },
      "outputs": [],
      "source": [
        "bert_truncated = train_data['trunc_bert_tokenized'].apply(lambda x: len(x) > bert_tokenizer.model_max_length).sum()\n",
        "uni_truncated = train_data['trunc_uni_tokenized'].apply(lambda x: len(x) > unix_tokenizer.model_max_length).sum()\n",
        "\n",
        "print(\"Sessions exceeding max token length after truncation:\")\n",
        "print(f\"  BERT:      {bert_truncated}\")\n",
        "print(f\"  UniXcoder: {uni_truncated}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WznxopbM3Cmk"
      },
      "source": [
        "## Task 3 : Model Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3lr_6zImdAb"
      },
      "source": [
        "### Preprocessing and Support Functions\n",
        "\n",
        "For model fine-tuning, we first prepare the dataset and define all required utility functions:\n",
        "\n",
        "- Data split: train, validation, test\n",
        "- Label mapping (label ↔ id)\n",
        "- Metric and plotting functions: accuracy, precision, recall, macro/per-class F1, session fidelity\n",
        "- Model pipeline definition for both pretrained and baseline architectures\n",
        "\n",
        "All key preprocessing and evaluation functions are provided in the following block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8QQCA6WpASB"
      },
      "outputs": [],
      "source": [
        "train_data_split, val_data_split = train_test_split(train_data, test_size=0.2, random_state=42)\n",
        "print(f\"Train set size:      {train_data_split.shape[0]:>4}\")\n",
        "print(f\"Validation set size: {val_data_split.shape[0]:>4}\")\n",
        "print(f\"Test set size:       {test_data.shape[0]:>4}\\n\")\n",
        "\n",
        "full_ds = DatasetDict({\n",
        "    \"train\": Dataset.from_pandas(train_data_split.reset_index(drop=True)),\n",
        "    \"valid\": Dataset.from_pandas(val_data_split.reset_index(drop=True)),\n",
        "    \"test\": Dataset.from_pandas(test_data.reset_index(drop=True)),\n",
        "})\n",
        "\n",
        "unique_labels = list(train_data.label.explode().unique())\n",
        "print(\"Unique labels:\")\n",
        "print(unique_labels)\n",
        "print()\n",
        "\n",
        "id2label = {i: label for i, label in enumerate(unique_labels)}\n",
        "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
        "\n",
        "print(\"Label to ID mapping:\")\n",
        "for k in label2id:\n",
        "    print(f\"  {k:18}: {label2id[k]}\")\n",
        "print()\n",
        "\n",
        "def convert_labels_to_ids(sample):\n",
        "    sample['label_id'] = [label2id[el] for el in sample[\"label\"]]\n",
        "    return sample\n",
        "\n",
        "encoded_dataset = full_ds.map(convert_labels_to_ids)\n",
        "display(pd.DataFrame([encoded_dataset]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMrc8dQG3Cmo"
      },
      "outputs": [],
      "source": [
        "def postprocess(predictions, labels):\n",
        "    predictions = predictions.detach().cpu().clone().numpy()\n",
        "    labels = labels.detach().cpu().clone().numpy()\n",
        "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
        "    true_predictions = [\n",
        "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    return true_labels, true_predictions\n",
        "\n",
        "def compute_metrics(full_predictions, full_labels):\n",
        "    flat_predictions = list(chain(*full_predictions))\n",
        "    flat_labels = list(chain(*full_labels))\n",
        "    metrics = {\n",
        "        \"token_accuracy\": accuracy_score(flat_labels, flat_predictions),\n",
        "        \"token_precision\": precision_score(flat_labels, flat_predictions, average='macro', zero_division=0),\n",
        "        \"token_recall\": recall_score(flat_labels, flat_predictions, average='macro', zero_division=0),\n",
        "        \"token_f1\": f1_score(flat_labels, flat_predictions, average='macro', zero_division=0),\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "def plot_stats(title, training_losses, validation_losses=None, best_epoch=None):\n",
        "    plt.style.use('seaborn-v0_8-paper')\n",
        "    colors = {\n",
        "        'training': '#2E86C1',\n",
        "        'validation': '#27AE60',\n",
        "        'best_epoch': '#E74C3C',\n",
        "    }\n",
        "    if validation_losses is not None:\n",
        "        fig, ((ax1, ax2)) = plt.subplots(1, 2, figsize=(5,3))\n",
        "    else:\n",
        "        fig, ax1 = plt.subplots(1, figsize=(5,4))\n",
        "\n",
        "    ax1.plot(training_losses, color=colors['training'], linewidth=2)\n",
        "    ax1.set_title('Training Loss', fontsize=12, pad=10)\n",
        "    ax1.set_ylabel('Loss', fontsize=10)\n",
        "    ax1.set_xlabel('Training Steps', fontsize=10)\n",
        "\n",
        "    if validation_losses is not None:\n",
        "        ax1.axvline(x=best_epoch, color=colors['best_epoch'], linestyle='--', alpha=0.8, label='Best Epoch')\n",
        "        ax2.plot(validation_losses, color=colors['validation'], linewidth=2)\n",
        "        ax2.axvline(x=best_epoch, color=colors['best_epoch'], linestyle='--', alpha=0.8, label='Best Epoch')\n",
        "        ax2.set_title('Validation Loss', fontsize=12, pad=10)\n",
        "        ax2.set_ylabel('Loss', fontsize=10)\n",
        "        ax2.set_xlabel('Training Steps', fontsize=10)\n",
        "        axs = [ax1, ax2]\n",
        "    else:\n",
        "        axs = [ax1]\n",
        "\n",
        "    for ax in axs:\n",
        "        ax.grid(True, linestyle='--', alpha=0.7)\n",
        "        ax.spines['top'].set_visible(False)\n",
        "        ax.spines['right'].set_visible(False)\n",
        "        if len(axs) > 1:\n",
        "            ax.legend(fontsize=8)\n",
        "    fig.suptitle(f'{title} - Training Losses', fontsize=14, y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def compute_per_class_f1(full_predictions, full_labels, labels):\n",
        "    flat_predictions = list(chain(*full_predictions))\n",
        "    flat_labels = list(chain(*full_labels))\n",
        "    report = classification_report(flat_labels, flat_predictions, output_dict=True, labels=labels, zero_division=0)\n",
        "    per_class_f1 = {tag: report[tag]['f1-score'] for tag in labels if tag in report}\n",
        "    return per_class_f1\n",
        "\n",
        "def plot_per_class_f1(per_class_f1_scores, title):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "\n",
        "    plt.style.use('seaborn-v0_8-paper')\n",
        "    tags = list(per_class_f1_scores.keys())\n",
        "    f1_scores = list(per_class_f1_scores.values())\n",
        "    sorted_tags, sorted_f1_scores = zip(*sorted(zip(tags, f1_scores), key=lambda x: x[0]))\n",
        "    colors = plt.cm.tab20(np.linspace(0, 1, len(sorted_tags)))\n",
        "\n",
        "    fig, ax = plt.subplots(1, figsize=(8, 6))\n",
        "    bars = ax.bar(sorted_tags, sorted_f1_scores, color=colors)\n",
        "\n",
        "    for bar, score in zip(bars, sorted_f1_scores):\n",
        "        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,\n",
        "                f\"{score:.2f}\", ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
        "\n",
        "    threshold_labels = {\n",
        "        0.4: \"Bad\",\n",
        "        0.6: \"Poor to Fair\",\n",
        "        0.8: \"Good\",\n",
        "        1.0: \"Excellent\"\n",
        "    }\n",
        "    for y, label in threshold_labels.items():\n",
        "        ax.axhline(y, color='red', linestyle='--', linewidth=1)\n",
        "        ax.text(\n",
        "            x=len(sorted_tags) - 0.5,\n",
        "            y=y + 0.01,\n",
        "            s=label,\n",
        "            fontsize=14,\n",
        "            color='red',\n",
        "            ha='right',\n",
        "            va='bottom'\n",
        "        )\n",
        "\n",
        "    ax.set_xlabel(\"MITRE Tactic Tag\", fontsize=18)\n",
        "    ax.set_ylabel(\"F1-score\", fontsize=18)\n",
        "    ax.set_title(f\"{title} - Per-Class F1-score on Test Set\", fontsize=18, fontweight='bold', pad=10)\n",
        "    ax.set_xticks(range(len(sorted_tags)))\n",
        "    ax.set_xticklabels(sorted_tags, rotation=45, ha='right', fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=18)\n",
        "    ax.grid(True, linestyle='--', alpha=0.7)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def calculate_session_fidelity(true_labels, predicted_labels):\n",
        "    correct_predictions = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == pred)\n",
        "    total_tokens = len(true_labels)\n",
        "    return correct_predictions / total_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrwCfUp53Cmo"
      },
      "outputs": [],
      "source": [
        "def pipeline(model_name, model, lr, n_train_epochs, title):\n",
        "    start_time = time.time()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
        "\n",
        "    # --- Tokenization and label alignment functions ---\n",
        "    def align_labels_with_tokens(labels, word_ids):\n",
        "        new_labels = []\n",
        "        current_word = None\n",
        "        for word_id in word_ids:\n",
        "            if word_id != current_word:\n",
        "                current_word = word_id\n",
        "                label = -100 if word_id is None else labels[word_id]\n",
        "                new_labels.append(label)\n",
        "            elif word_id is None:\n",
        "                new_labels.append(-100)\n",
        "            else:\n",
        "                label = labels[word_id]\n",
        "                new_labels.append(label)\n",
        "        return new_labels\n",
        "\n",
        "    def tokenize_and_align_labels(samples):\n",
        "        split_sentences = [sentence.split(\" \") for sentence in samples[\"trunc_session\"]]\n",
        "        tokenized_inputs = tokenizer(\n",
        "            split_sentences,\n",
        "            truncation=True,\n",
        "            is_split_into_words=True\n",
        "        )\n",
        "        all_labels = samples[\"label_id\"]\n",
        "        new_labels = []\n",
        "        for i, labels in enumerate(all_labels):\n",
        "            word_ids = tokenized_inputs.word_ids(i)\n",
        "            aligned_labels = align_labels_with_tokens(labels, word_ids)\n",
        "            new_labels.append(aligned_labels)\n",
        "        tokenized_inputs[\"labels\"] = new_labels\n",
        "        return tokenized_inputs\n",
        "\n",
        "    # --- Dataset tokenization ---\n",
        "    original_columns = encoded_dataset[\"train\"].column_names\n",
        "    tokenized_datasets = encoded_dataset.map(\n",
        "        tokenize_and_align_labels,\n",
        "        batched=True,\n",
        "        remove_columns=original_columns,\n",
        "    )\n",
        "\n",
        "    # --- Dataloaders ---\n",
        "    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "    train_dataloader = DataLoader(\n",
        "        tokenized_datasets[\"train\"],\n",
        "        shuffle=True,\n",
        "        collate_fn=data_collator,\n",
        "        batch_size=16,\n",
        "    )\n",
        "    eval_dataloader = DataLoader(\n",
        "        tokenized_datasets[\"valid\"], collate_fn=data_collator, batch_size=16\n",
        "    )\n",
        "    test_dataloader = DataLoader(\n",
        "        tokenized_datasets[\"test\"], collate_fn=data_collator, batch_size=16\n",
        "    )\n",
        "\n",
        "    # --- Optimizer and LR scheduler ---\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    num_update_steps_per_epoch = len(train_dataloader)\n",
        "    num_training_steps = n_train_epochs * num_update_steps_per_epoch\n",
        "    lr_scheduler = get_scheduler(\n",
        "        \"linear\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_training_steps,\n",
        "    )\n",
        "\n",
        "    # --- Training loop ---\n",
        "\n",
        "    best_val_loss, best_epoch = np.inf, 0\n",
        "    best_model = deepcopy(model)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    progress_bar = tqdm(range(num_training_steps))\n",
        "    val_losses, train_losses = [], []\n",
        "\n",
        "    for epoch in range(n_train_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch in train_dataloader:\n",
        "            batch = {key:value.to(device) for key, value in batch.items()}\n",
        "            outputs = model(input_ids=batch[\"input_ids\"],\n",
        "                            attention_mask=batch[\"attention_mask\"],\n",
        "                            labels=batch[\"labels\"])\n",
        "            loss = outputs.loss\n",
        "            train_loss += loss.detach().cpu().clone().numpy()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            progress_bar.update(1)\n",
        "        train_losses.append(train_loss/len(train_dataloader))\n",
        "\n",
        "        # --- Validation at end of each epoch ---\n",
        "        model.eval()\n",
        "        full_predictions, full_labels = [], []\n",
        "        val_loss = 0\n",
        "        for batch in eval_dataloader:\n",
        "            batch = {key:value.to(device) for key, value in batch.items()}\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**batch)\n",
        "            val_loss += outputs.loss.detach().cpu().clone().numpy()\n",
        "            predictions = outputs.logits.argmax(dim=-1)\n",
        "            labels = batch[\"labels\"]\n",
        "            true_predictions, true_labels = postprocess(predictions, labels)\n",
        "            full_predictions += true_predictions\n",
        "            full_labels += true_labels\n",
        "        val_loss = val_loss/len(eval_dataloader)\n",
        "        val_losses.append(val_loss)\n",
        "        metrics = compute_metrics(full_predictions, full_labels)\n",
        "        if val_loss <= best_val_loss:\n",
        "            best_epoch = epoch\n",
        "            best_val_loss = val_loss\n",
        "            best_model = deepcopy(model)\n",
        "\n",
        "    # --- Plot training and validation loss ---\n",
        "    plot_stats(title, train_losses, validation_losses=val_losses, best_epoch=best_epoch)\n",
        "\n",
        "    # --- Final evaluation on test set ---\n",
        "    model.eval()\n",
        "    full_predictions, full_labels = [], []\n",
        "    for batch in test_dataloader:\n",
        "        batch = {key:value.to(device) for key, value in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "        predictions = outputs.logits.argmax(dim=-1)\n",
        "        labels = batch[\"labels\"]\n",
        "        true_predictions, true_labels = postprocess(predictions, labels)\n",
        "        full_predictions += true_predictions\n",
        "        full_labels += true_labels\n",
        "\n",
        "    # --- Compute and print metrics ---\n",
        "    test_metrics = compute_metrics(full_predictions, full_labels)\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(title + \" - Evaluation Metrics\")\n",
        "    print(\"=\"*50)\n",
        "    for metric, value in test_metrics.items():\n",
        "        print(f\"{metric.replace('_', ' ').capitalize():<25}: {value*100:6.2f}%\")\n",
        "\n",
        "    # --- Per-class F1-score plot ---\n",
        "    print(\"\\nPer-class f1-score: reports the results in a barplot.\")\n",
        "    per_class_f1_scores = compute_per_class_f1(full_predictions, full_labels, unique_labels)\n",
        "    plot_per_class_f1(per_class_f1_scores, title)\n",
        "    # --- Session fidelity calculation ---\n",
        "    session_fidelities = [calculate_session_fidelity(true_sess_labels, pred_sess_labels)\n",
        "                          for true_sess_labels, pred_sess_labels in zip(full_labels, full_predictions)]\n",
        "    average_fidelity = np.mean(session_fidelities)\n",
        "\n",
        "    print(f\"\\nAverage Session Fidelity: {average_fidelity:.4f}\")\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    test_metrics['time_seconds'] = elapsed\n",
        "    return {title: test_metrics}, {title: per_class_f1_scores}, best_model\n",
        "\n",
        "results = {}\n",
        "dict_f1_scores = {}\n",
        "\n",
        "n_epochs = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kk_JhRuGknQq"
      },
      "source": [
        "### Fine-Tuning on Pretrained BERT\n",
        "\n",
        "**BERT Pretrained Results**\n",
        "\n",
        "- **Token classification accuracy:** ~83%\n",
        "- **Macro F1-score:** ~61%\n",
        "- **Macro precision:** ~56%\n",
        "- **Macro recall:** ~73%\n",
        "- **Average session fidelity:** ~0.8\n",
        "\n",
        "*See the F1-score bar chart below for detailed per-class performance.*\n",
        "\n",
        "Performance is **Excellent** for `Discovery`, `Persistance` and `Execution`, **Good** for `Defence Evasion`.  \n",
        "Results are **lower** for `Not Malicious Yet` and `Other`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9BD9O-q3Cmo"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "model_name = \"google-bert/bert-base-uncased\"\n",
        "bert_pretrained = AutoModelForTokenClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path=model_name,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "\n",
        "results_dict, f1_scores_dict, _ = pipeline(\n",
        "    model_name,\n",
        "    bert_pretrained,\n",
        "    lr=1e-5,\n",
        "    n_train_epochs=n_epochs,\n",
        "    title=\"BERT Pretrained\"\n",
        ")\n",
        "\n",
        "results.update(results_dict)\n",
        "dict_f1_scores.update(f1_scores_dict)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YBLFUMlt6R-"
      },
      "source": [
        "### Fine-Tuning on Naked BERT\n",
        "\n",
        "**BERT Naked Results**\n",
        "\n",
        "- **Token classification accuracy:** ~74~%\n",
        "- **Macro F1-score:** ~47~%\n",
        "- **Macro precision:** ~45%\n",
        "- **Macro recall:** ~58%\n",
        "- **Average session fidelity:** ~0.75\n",
        "\n",
        "*See the F1-score bar chart below for detailed per-class performance.*\n",
        "\n",
        "Compared to the pretrained model, the naked BERT achieves significantly lower macro F1 scores.\n",
        "As expected, training from scratch with limited data leads to poor generalization, especially on minority classes.\n",
        "\n",
        "Performance by class follows similar trends:  \n",
        "- **Higher** for `Discovery` and `Persistence`  \n",
        "- **Lower** for `Defense Evasion`,`Impact`,`Not Malicious Yet` and `Other`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8g3H2sJuPiK"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "model_name = \"google-bert/bert-base-uncased\"\n",
        "config = AutoConfig.from_pretrained(\n",
        "    pretrained_model_name_or_path=model_name,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "bert_naked = AutoModelForTokenClassification.from_config(config)\n",
        "\n",
        "results_dict, f1_scores_dict, _ = pipeline(\n",
        "    model_name,\n",
        "    bert_naked,\n",
        "    lr=1e-5,\n",
        "    n_train_epochs=n_epochs,\n",
        "    title=\"BERT Naked\"\n",
        ")\n",
        "\n",
        "results.update(results_dict)\n",
        "dict_f1_scores.update(f1_scores_dict)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_0skFRKue5G"
      },
      "source": [
        "### Fine-Tuning on Pretrained UniXcoder\n",
        "\n",
        "**UniXcoder Pretrained Results**\n",
        "\n",
        "- **Token classification accuracy:** ~88%\n",
        "- **Macro F1-score:** ~76%\n",
        "- **Macro precision:** ~71%\n",
        "- **Macro recall:** ~88%\n",
        "- **Average session fidelity:** ~0.84\n",
        "\n",
        "*See the F1-score bar chart below for detailed per-class performance.*\n",
        "\n",
        "Compared to BERT, UniXcoder achieves better results.  \n",
        "This suggests that pre-training on code provides an advantage for this token classification task.\n",
        "\n",
        "Performance trends by class remain consistent, with best results for `Defense Evasion`,`Discovery`,`Execution` and `Persistence`, and lower F1 for `Not Malicious Yet`, `Impact` and `Other`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zy4FeUQvcq4"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "model_name = \"microsoft/unixcoder-base\"\n",
        "\n",
        "unixcoder_pretrained = AutoModelForTokenClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path=model_name,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "\n",
        "results_dict, f1_scores_dict, _ = pipeline(\n",
        "    model_name,\n",
        "    unixcoder_pretrained,\n",
        "    lr=1e-5,\n",
        "    n_train_epochs=n_epochs,\n",
        "    title=\"UniXcoder Pretrained\"\n",
        ")\n",
        "\n",
        "results.update(results_dict)\n",
        "dict_f1_scores.update(f1_scores_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNSfuzoowoqa"
      },
      "source": [
        "### Fine-Tuning on SecureShellBert\n",
        "\n",
        "**SecureShellBert Results**\n",
        "\n",
        "- **Token classification accuracy:** ~86%\n",
        "- **Macro F1-score:** ~69%\n",
        "- **Macro precision:** ~65%\n",
        "- **Macro recall:** ~79%\n",
        "- **Average session fidelity:** ~0.85\n",
        "\n",
        "*See the F1-score bar chart below for detailed per-class performance.*\n",
        "\n",
        "Compared to UniXcoder and BERT, SecureShellBert achieves slightly lower F1-score.  \n",
        "Domain adaptation on BASH commands provides an advantage on security-relevant classes but no significant boost compared to UniXcoder, that perform better.\n",
        "\n",
        "Performance by class again shows strongest results for `Discovery`, `Execution` and `Persistence`, with lower scores for `Not Malicious Yet`, `Impact` and `Other`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdX3UFg5w3R-"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "model_name = \"SmartDataPolito/SecureShellBert\"\n",
        "\n",
        "secureshellbert = AutoModelForTokenClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path=model_name,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "\n",
        "results_dict, f1_scores_dict, _ = pipeline(\n",
        "    model_name,\n",
        "    secureshellbert,\n",
        "    lr=1e-5,\n",
        "    n_train_epochs=n_epochs,\n",
        "    title=\"SecureShellBert\"\n",
        ")\n",
        "\n",
        "results.update(results_dict)\n",
        "dict_f1_scores.update(f1_scores_dict)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbxGsDqAknQt"
      },
      "source": [
        "### Global Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arHwNfLnknQu"
      },
      "outputs": [],
      "source": [
        "def result_graph(results,dict_f1_scores):\n",
        "    df = pd.DataFrame(results).T\n",
        "    df = df.drop(columns=['time_seconds'], errors='ignore')\n",
        "\n",
        "    def highlight_max(s, props='font-weight: bold'):\n",
        "        return [props if v == s.max() else '' for v in s]\n",
        "\n",
        "    styled = df.style.format('{:.2%}').apply(highlight_max, axis=0)\n",
        "\n",
        "    print(\"Final results:\")\n",
        "    display(styled)\n",
        "\n",
        "    results = dict_f1_scores\n",
        "    tags = sorted(list(next(iter(results.values())).keys()))\n",
        "    n_tags = len(tags)\n",
        "    n_models = len(results)\n",
        "\n",
        "    x = np.arange(n_tags)\n",
        "    width = 0.18\n",
        "    colors = plt.cm.Set2(np.linspace(0, 1, n_models))\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(18, 6))\n",
        "\n",
        "    for i, (model_name, f1_scores) in enumerate(results.items()):\n",
        "        scores = [f1_scores[tag] for tag in tags]\n",
        "        ax.bar(x + i * width - (width * (n_models - 1) / 2), scores, width, label=model_name, color=colors[i])\n",
        "\n",
        "    threshold_labels = {\n",
        "        0.4: \"Bad\",\n",
        "        0.6: \"Poor to Fair\",\n",
        "        0.8: \"Good\",\n",
        "        1.0: \"Excellent\"\n",
        "    }\n",
        "    for y, label in threshold_labels.items():\n",
        "        ax.axhline(y, color='red', linestyle='--', linewidth=1)\n",
        "        ax.text(\n",
        "            x=n_tags - 0.5,\n",
        "            y=y - 0.02,\n",
        "            s=label,\n",
        "            fontsize=14,\n",
        "            color='red',\n",
        "            ha='left',\n",
        "            va='top'\n",
        "        )\n",
        "\n",
        "    ax.set_xlabel('MITRE Tactic Tag', fontsize=18)\n",
        "    ax.set_ylabel('F1-score', fontsize=18)\n",
        "    ax.set_title('Per-Class F1-score Comparison', fontsize=18, fontweight='bold', pad=10)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(tags, rotation=45, ha='right', fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=18)\n",
        "    ax.grid(True, linestyle='--', alpha=0.7)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "\n",
        "    box = ax.get_position()\n",
        "    ax.set_position([box.x0, box.y0 + 0.15, box.width, box.height * 0.75])\n",
        "\n",
        "    ax.legend(\n",
        "        title='Models',\n",
        "        title_fontsize=18,\n",
        "        fontsize=18,\n",
        "        loc='lower center',\n",
        "        bbox_to_anchor=(1.2, -0.3),\n",
        "\n",
        "    )\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def plot_training_times(results):\n",
        "    df = pd.DataFrame(results).T\n",
        "\n",
        "    plt.figure(figsize=(6,4))\n",
        "    bar_width = 0.6\n",
        "    plt.bar(df.index, df['time_seconds'], color='slateblue', edgecolor='black', width=bar_width)\n",
        "    for idx, val in enumerate(df['time_seconds']):\n",
        "        plt.text(idx, val -8, f\"{val:.0f}s\", ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "    plt.ylabel(\"Time (seconds)\", fontsize=18)\n",
        "    plt.title(\"Training & Evaluation Time by Model\", fontsize=18, fontweight='bold')\n",
        "    plt.xticks(rotation=30, ha='right', fontsize=14)\n",
        "    plt.yticks(fontsize=18)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "result_graph(results,dict_f1_scores)\n",
        "print()\n",
        "plot_training_times(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWcxQYCAkf_o"
      },
      "source": [
        "### Layer Freezing Experiments on UniXcoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vwe6QAp8u5R5"
      },
      "source": [
        "#### Fine-Tuning Only the Last 2 Layers + Classifier Head (UniXcoder)\n",
        "\n",
        "**Frozen UniXcoder (Last 2 Layers + Classifier) Results**\n",
        "\n",
        "- **Token classification accuracy:** ~78%\n",
        "- **Macro F1-score:** ~54%\n",
        "- **Macro precision:** ~49%\n",
        "- **Macro recall:** ~74%\n",
        "- **Average session fidelity:** ~0.75\n",
        "\n",
        "*See the F1-score bar chart below for detailed per-class performance.*\n",
        "\n",
        "Only the classification head parameters and the last two encoder layers were updated; all other encoder layers were frozen. The learning rate was kept at 1e-5, though in some scenarios a higher LR may be beneficial when fewer parameters are being updated.\n",
        "\n",
        "Compared to full fine-tuning, updating only the last two encoder layers and the classifier results in a moderate drop in macro F1, but noticeably faster training.\n",
        "For some classes like `Discovery`, `Execution`, and `Persistence`, performance remains excellent; however, for others there is a significant drop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_50QX-Eu2pV"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "best_model_name = \"microsoft/unixcoder-base\"\n",
        "\n",
        "best_model = AutoModelForTokenClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path=best_model_name,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "\n",
        "\n",
        "# Freeze all layers in the base model\n",
        "for param in best_model.base_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze only the last 2 encoder layers\n",
        "for param in best_model.base_model.encoder.layer[-2:].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Unfreeze the classifier head\n",
        "for param in best_model.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Print names of trainable parameters\n",
        "total_params = sum(p.numel() for p in best_model.parameters())\n",
        "trainable_params = [(name, p.numel()) for name, p in best_model.named_parameters() if p.requires_grad]\n",
        "trainable_count = sum(n for _, n in trainable_params)\n",
        "\n",
        "print(f\"Total model parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_count:,} ({trainable_count/total_params:.2%} of total)\\n\")\n",
        "print(\"Trainable parameter tensors:\")\n",
        "for idx, (name, num) in enumerate(trainable_params, 1):\n",
        "    print(f\"{idx:2d}. {name:<60} ({num:,} params)\")\n",
        "\n",
        "results_dict, f1_scores_dict, _ = pipeline(\n",
        "    best_model_name,\n",
        "    best_model,\n",
        "    lr=1e-5,\n",
        "    n_train_epochs=n_epochs,\n",
        "    title=\"Frz-[2L+Clf] (LR=1e-5)\"\n",
        ")\n",
        "\n",
        "results.update(results_dict)\n",
        "dict_f1_scores.update(f1_scores_dict)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GA2qmGCkImz"
      },
      "source": [
        "#### Fine-Tuning Only the Classification Head (UniXcoder, LR=1e-5)\n",
        "\n",
        "**Frozen UniXcoder (Classification Head Only) Results**\n",
        "\n",
        "- **Token classification accuracy:** ~43%\n",
        "- **Macro F1-score:** ~17%\n",
        "- **Macro precision:** ~19%\n",
        "- **Macro recall:** ~18%\n",
        "- **Average session fidelity:** ~0.34\n",
        "\n",
        "*See the F1-score bar chart below for detailed per-class performance.*\n",
        "\n",
        "Only the classification head (the output layer for token classification) was updated; all encoder layers were frozen. Training is significantly faster with so few parameters updated, and the learning rate was kept at 1e-5.\n",
        "Compared to tuning more layers, there is a substantial drop in macro F1 and session fidelity, confirming that updating only the classification head is not enough for strong generalization.\n",
        "Per-class F1-scores highlight the limitations of this approach, especially for minority or ambiguous classes.\n",
        "\n",
        "In the next experiment, we will try a higher learning rate (1e-4) to see if it helps convergence or improves the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_S1tfTKm3s3s"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "model_name = \"microsoft/unixcoder-base\"\n",
        "\n",
        "model_2layer_classifier = AutoModelForTokenClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path=model_name,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "\n",
        "# Freeze all layers in the base model\n",
        "for param in model_2layer_classifier.base_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze ONLY the classifier head\n",
        "for param in model_2layer_classifier.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Show summary of trainable parameters\n",
        "total_params = sum(p.numel() for p in model_2layer_classifier.parameters())\n",
        "trainable_params = [(name, p.numel()) for name, p in model_2layer_classifier.named_parameters() if p.requires_grad]\n",
        "trainable_count = sum(n for _, n in trainable_params)\n",
        "\n",
        "print(f\"Total model parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_count:,} ({trainable_count/total_params:.2%} of total)\\n\")\n",
        "print(\"Trainable parameter tensors:\")\n",
        "for idx, (name, num) in enumerate(trainable_params, 1):\n",
        "    print(f\"{idx:2d}. {name:<60} ({num:,} params)\")\n",
        "\n",
        "results_dict, f1_scores_dict, _ = pipeline(\n",
        "    model_name,\n",
        "    model_2layer_classifier,\n",
        "    lr=1e-5,\n",
        "    n_train_epochs=n_epochs,\n",
        "    title=\"Frz-[Clf] (LR=1e-5)\"\n",
        ")\n",
        "\n",
        "results.update(results_dict)\n",
        "dict_f1_scores.update(f1_scores_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkYbYnYzj0eN"
      },
      "source": [
        "#### Fine-Tuning Only the Classification Head (UniXcoder, LR=1e-4)\n",
        "\n",
        "**Frozen UniXcoder (Classification Head Only, LR=1e-4) Results**\n",
        "\n",
        "- **Token classification accuracy:** ~70%\n",
        "- **Macro F1-score:** ~40%\n",
        "- **Macro precision:** ~38%\n",
        "- **Macro recall:** ~59%\n",
        "- **Average session fidelity:** 0.68\n",
        "\n",
        "*See the F1-score bar chart below for detailed per-class performance.*\n",
        "\n",
        "Only the classifier head (the output layer for token classification) was updated; all encoder layers were frozen. With this setting, training is extremely fast since only the output head is updated.\n",
        "A higher learning rate (1e-4) was used to improve convergence when freezing the layers.\n",
        "\n",
        "Compared to full or partial fine-tuning, performance still drops—especially for macro F1 and session fidelity—highlighting that the model’s expressiveness is limited when only the classifier head is trainable.\n",
        "However, using a higher learning rate can partially recover performance, achieving moderate accuracy and F1 scores, but results remain substantially below those of models with more layers updated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJkGttS7knQv"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "model_name = \"microsoft/unixcoder-base\"\n",
        "\n",
        "model_classifier = AutoModelForTokenClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path=model_name,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "\n",
        "# Freeze all layers in the base model\n",
        "for param in model_classifier.base_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze ONLY the classifier head\n",
        "for param in model_classifier.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Display summary of trainable parameters\n",
        "total_params = sum(p.numel() for p in model_classifier.parameters())\n",
        "trainable_params = [(name, p.numel()) for name, p in model_classifier.named_parameters() if p.requires_grad]\n",
        "trainable_count = sum(n for _, n in trainable_params)\n",
        "\n",
        "print(f\"Total model parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_count:,} ({trainable_count/total_params:.2%} of total)\\n\")\n",
        "print(\"Trainable parameter tensors:\")\n",
        "for idx, (name, num) in enumerate(trainable_params, 1):\n",
        "    print(f\"{idx:2d}. {name:<60} ({num:,} params)\")\n",
        "\n",
        "\n",
        "results_dict, f1_scores_dict, _ = pipeline(\n",
        "    model_name,\n",
        "    model_classifier,\n",
        "    lr=1e-4,  # Higher LR for faster convergence with small head\n",
        "    n_train_epochs=n_epochs,\n",
        "    title=\"Frz-[Clf] (LR=1e-4)\"\n",
        ")\n",
        "\n",
        "results.update(results_dict)\n",
        "dict_f1_scores.update(f1_scores_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Scl681IC4UPI"
      },
      "source": [
        "#### Parameter Analysis & Results\n",
        "\n",
        "When all layers except the classifier were frozen, only 5,380 parameters (the classification head) were trainable.\n",
        "\n",
        "When fine-tuning the last two layers plus the classifier, 14.2 million parameters were trainable.\n",
        "\n",
        "The full base model contains about 125 million parameters.\n",
        "\n",
        "**Summary**\n",
        "- **Freezing all layers except the last two encoder layers and classifier**:  \n",
        "    Accuracy and F1-score decrease by around 10% and 22%, respectively.\n",
        "- **Freezing all layers except the classifier**:  \n",
        "    Accuracy and F1-score decrease by around 18% and 31%, respectively.\n",
        "- **Training speed**:  \n",
        "    Training is significantly faster as fewer parameters are updated, but performance loss is substantial compared to full fine-tuning, especially for rare and complex classes.\n",
        "\n",
        "See the plot below for a time comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfJ4xqeJknQw"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "\n",
        "for param in model_classifier.base_model.parameters():\n",
        "    param.requires_grad = False\n",
        "    count += param.numel()\n",
        "print(f\"Number of trainable parameters in the base model: {count:.2e}\")\n",
        "\n",
        "count_last_two = 0\n",
        "for param in model_classifier.base_model.encoder.layer[-2:].parameters():\n",
        "    param.requires_grad = True\n",
        "    count_last_two += param.numel()\n",
        "\n",
        "count_classifier = 0\n",
        "# The classification head (the token classifier) also needs to be trainable\n",
        "for param in model_classifier.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "    count_classifier += param.numel()\n",
        "\n",
        "print(f\"Number of trainable parameters in the last two layers + classifier: {count_last_two + count_classifier:.2e}\")\n",
        "print(f\"Number of trainable parameters in the classifier: {count_classifier:.2e}\")\n",
        "\n",
        "selected_titles = [\n",
        "    \"UniXcoder Pretrained\",\n",
        "    \"Frz-[2L+Clf] (LR=1e-5)\",\n",
        "    \"Frz-[Clf] (LR=1e-5)\",\n",
        "    \"Frz-[Clf] (LR=1e-4)\"\n",
        "]\n",
        "\n",
        "filtered_results = {k: v for k, v in results.items() if k in selected_titles}\n",
        "filtered_f1_scores = {k: v for k, v in dict_f1_scores.items() if k in selected_titles}\n",
        "\n",
        "result_graph(filtered_results,filtered_f1_scores)\n",
        "plot_training_times(filtered_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEpys7j1knQw"
      },
      "source": [
        "## Task 4: Inference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RSSqrsT_N4H"
      },
      "source": [
        "### Preprocessing and dataset analysis\n",
        "In this section, we will use the best fine-tuned model to predict MITRE tags on unseen inference sessions.\n",
        "We will preprocess the sessions by truncating words longer than 30 characters to avoid tokenization issues that could arise with extremely long tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWoYcIKQknQw"
      },
      "outputs": [],
      "source": [
        "dataset_url = 'https://github.com/PierfrancescoElia/AICyberLabs/raw/refs/heads/main/Lab03/cyberlab.bigcsv'\n",
        "cyberlab_df = pd.read_csv(dataset_url)\n",
        "def truncate_long_words(session):\n",
        "    max_length=30\n",
        "    truncated = ' '.join([w[:max_length] for w in session.split(\" \")])\n",
        "    return truncated\n",
        "\n",
        "\n",
        "cyberlab_df['trunc_session'] = cyberlab_df['session'].apply(truncate_long_words)\n",
        "\n",
        "dataset = Dataset.from_pandas(cyberlab_df.reset_index(drop=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQ0-RzUhknQw"
      },
      "outputs": [],
      "source": [
        "def inference(model_name, model):\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
        "\n",
        "    def tokenize_and_align_labels(samples):\n",
        "        split_sentences = [sentence.split(\" \") for sentence in samples[\"trunc_session\"]]\n",
        "\n",
        "        tokenized_inputs = tokenizer(\n",
        "            split_sentences,\n",
        "            truncation=True,\n",
        "            is_split_into_words=True,\n",
        "            max_length=1024,\n",
        "        )\n",
        "\n",
        "        all_word_ids = []\n",
        "        for i in range(len(split_sentences)):\n",
        "            all_word_ids.append(tokenized_inputs.word_ids(batch_index=i))\n",
        "        tokenized_inputs[\"word_ids\"] = all_word_ids\n",
        "        return tokenized_inputs\n",
        "\n",
        "    original_columns = dataset.column_names\n",
        "    tokenized_datasets = dataset.map(\n",
        "        tokenize_and_align_labels,\n",
        "        batched=True,\n",
        "        remove_columns=original_columns,\n",
        "    )\n",
        "\n",
        "    class DataCollatorWithWordIds:\n",
        "        def __init__(self, tokenizer):\n",
        "            self.tokenizer = tokenizer\n",
        "\n",
        "        def __call__(self, features):\n",
        "            word_ids = [feature[\"word_ids\"] for feature in features]\n",
        "\n",
        "            features_without_word_ids = [{k: v for k, v in f.items() if k != \"word_ids\"} for f in features]\n",
        "\n",
        "            batch = self.tokenizer.pad(\n",
        "                features_without_word_ids,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "\n",
        "            batch[\"word_ids\"] = word_ids\n",
        "\n",
        "            return batch\n",
        "\n",
        "    data_collator = DataCollatorWithWordIds(tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        tokenized_datasets,\n",
        "        shuffle=False,\n",
        "        collate_fn=data_collator,\n",
        "        batch_size=16,\n",
        "    )\n",
        "\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    preds = []\n",
        "\n",
        "    for count, batch in enumerate(tqdm(dataloader)):\n",
        "        batch = {key:(value.to(device) if key != 'word_ids' else value) for key, value in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=batch[\"input_ids\"],\n",
        "                            attention_mask=batch[\"attention_mask\"],\n",
        "                            )\n",
        "\n",
        "        predictions = outputs.logits.argmax(dim=-1)\n",
        "        batch_preds = predictions.cpu().numpy()\n",
        "        batch_words_ids = batch['word_ids']\n",
        "\n",
        "        for pred_seq, word_id_seq in zip(batch_preds, batch_words_ids):\n",
        "            word_preds = []\n",
        "            previous_word_idx = None\n",
        "            for pred, word_idx in zip(pred_seq, word_id_seq):\n",
        "                if word_idx is None:\n",
        "                    continue\n",
        "                if word_idx != previous_word_idx:\n",
        "                    word_preds.append(pred)\n",
        "                previous_word_idx = word_idx\n",
        "            preds.append(word_preds)\n",
        "\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JorkOoIb_N4H"
      },
      "source": [
        "### Inference step\n",
        "We re-train and fine-tune our best-performing model (`microsoft/unixcoder-base`) using the training data. This model will then be used to predict MITRE tags on the inference dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_VTvGRnknQx"
      },
      "outputs": [],
      "source": [
        "model_name = \"microsoft/unixcoder-base\"\n",
        "\n",
        "unixcoder_pretrained = AutoModelForTokenClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path=model_name,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "_, _, best_model = pipeline(\n",
        "    model_name,\n",
        "    unixcoder_pretrained,\n",
        "    lr=1e-5,\n",
        "    n_train_epochs=n_epochs,\n",
        "    title=\"UniXcoder Pretrained\"\n",
        ")\n",
        "\n",
        "preds = inference(model_name, best_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmbPQ5jD_N4I"
      },
      "source": [
        "### Inspection on two samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6qBKjPWknQx"
      },
      "outputs": [],
      "source": [
        "sample_indices = [1, 5800]\n",
        "for i in sample_indices:\n",
        "    words = cyberlab_df.iloc[i]['trunc_session'].split(\" \")\n",
        "    len_seq = len(words)\n",
        "    len_preds = len(preds[i])\n",
        "\n",
        "    print(f\"Session {i+1} - Words: {len_seq}, Predictions: {len_preds}\\n\")\n",
        "\n",
        "    print(\"Session:\")\n",
        "    print(cyberlab_df.iloc[i]['trunc_session'])\n",
        "    print(\"\\nPredicted tags:\")\n",
        "\n",
        "    tags = [id2label[p] for p in preds[i]]\n",
        "\n",
        "    for w, t in zip(words[7:15], tags[7:15]):\n",
        "        print(f\"{w:<25} {t}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dMU0Ata_N4I"
      },
      "source": [
        "### Command-Tag Frequency Analysis\n",
        "We analyzed the model's predictions focusing on the commands `cat`, `grep`, `echo`, and `rm`. The table below reports the frequency (in percentage) of each predicted MITRE tag associated with the selected commands.\n",
        "\n",
        "Based on the frequency analysis:\n",
        "- **`cat`** is predominantly associated with the `Discovery` tactic, consistent with its use in system information gathering.\n",
        "- **`grep`** is almost exclusively mapped to `Discovery`, reflecting its common role in filtering and extracting information.\n",
        "- **`echo`** shows a mix of tags, primarily tied to `Discovery`, `Persistence`, and `Execution`, depending on how it is used.\n",
        "- **`rm`** is associated mainly with `Discovery` and `Defense Evasion`, which aligns with its use in both information gathering and destructive actions (e.g., deleting log files to evade detection).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_LSPssMknQx"
      },
      "outputs": [],
      "source": [
        "commands = ['cat', 'grep', 'echo', 'rm']\n",
        "tag_freq = {cmd: Counter() for cmd in commands}\n",
        "\n",
        "sample_sessions = {(cmd, tag): [] for cmd in commands for tag in unique_labels}\n",
        "\n",
        "for i in range(len(preds)):\n",
        "    words = cyberlab_df['trunc_session'][i].split(\" \")\n",
        "    preds_i = preds[i]\n",
        "    for pos, (word, pred) in enumerate(zip(words, preds_i)):\n",
        "        if word in commands:\n",
        "            tag_freq[word][id2label[pred]] += 1\n",
        "            sample_sessions[(word,id2label[pred])].append((i,pos))\n",
        "\n",
        "rows = []\n",
        "for cmd, freq in tag_freq.items():\n",
        "    total = sum(freq.values())\n",
        "    for tag in unique_labels:\n",
        "        count = freq.get(tag, 0)\n",
        "        frequency = count / total if total > 0 else 0\n",
        "        rows.append({'Command': cmd, 'Tag': tag, 'Frequency (%)': f\"{frequency * 100:.2f}\"})\n",
        "\n",
        "df_cmd_tag = pd.DataFrame(rows)\n",
        "df_pivot = df_cmd_tag.pivot(index='Command', columns='Tag', values='Frequency (%)')\n",
        "display(df_pivot)\n",
        "\n",
        "for (command, tag), examples in sample_sessions.items():\n",
        "    if len(examples) > 0:\n",
        "        print(f\"Command: {command}, Predicted Tag: {tag}\")\n",
        "        print(\"Example Sessions:\")\n",
        "\n",
        "        sentence_id, position_id = examples[0]\n",
        "\n",
        "        session_words = cyberlab_df.iloc[sentence_id]['trunc_session'].split(\" \")\n",
        "        snippet = ' '.join(session_words[position_id:position_id+30])\n",
        "        print(f\"- [...]{snippet}[...] (Session ID: {sentence_id}, Position: {position_id})\\n\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2UJe90G_N4J"
      },
      "source": [
        "### Fingerprint Extraction and Analysis\n",
        "\n",
        "Each session's sequence of MITRE tactic predictions is considered a *fingerprint*. Sessions sharing the same fingerprint can be grouped and analyzed together.\n",
        "We:\n",
        "- Extract the set of unique fingerprints.\n",
        "- Assign a fingerprint ID based on the order of first appearance.\n",
        "- Count how many sessions correspond to each fingerprint per day."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpylvCblknQx"
      },
      "outputs": [],
      "source": [
        "print(\"Total number of predictions:\")\n",
        "print(len(preds)) # Total number of predictions\n",
        "unique_fingerprints = set(tuple(pred) for pred in preds)\n",
        "print(\"Unique fingerprints:\")\n",
        "print(len(unique_fingerprints)) # Total number of unique fingerprints\n",
        "\n",
        "cyberlab_df['fingerprint'] = [tuple(pred) for pred in preds]\n",
        "\n",
        "fingerprint_df = (\n",
        "    cyberlab_df.sort_values(by='timestamps_statements')\n",
        "    .groupby('fingerprint', as_index=False)\n",
        "    .first()[['fingerprint','timestamps_statements']]\n",
        ").sort_values(by='timestamps_statements')\n",
        "\n",
        "fingerprint_df.reset_index(drop=True, inplace=True)\n",
        "fingerprint_df\n",
        "\n",
        "fingerprint_to_id = {tuple(f): idx for idx, f in enumerate(fingerprint_df['fingerprint'])}\n",
        "\n",
        "pairs = [\n",
        "    (pd.to_datetime(row.timestamps_statements).date(), fingerprint_to_id[tuple(row.fingerprint)]) for row in cyberlab_df.itertuples(index=False)\n",
        "]\n",
        "dict_day_fingerprint = Counter(pairs)\n",
        "sorted_list = sorted(dict_day_fingerprint.items(), key=lambda x: (x[0][0], x[0][1])) ## (date, fingerprint_id): num_occurrences\n",
        "\n",
        "for i in sorted_list[:10]:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRshuLX-_N4J"
      },
      "source": [
        "### Interpretation of Fingerprint Patterns\n",
        "\n",
        "The scatter plot illustrates the temporal distribution of session fingerprints, offering meaningful insights into the dynamics of system activity over time. Each point represents a specific fingerprint on a given day, where the size and color intensity reflect the number of sessions associated with that fingerprint. Larger, darker red points correspond to fingerprints with high session counts, while smaller, blue points represent those with relatively few sessions.\n",
        "\n",
        "Several observations emerge from the analysis of this visualization. Some fingerprints, particularly those with lower indices, persist consistently throughout the entire collection period. This temporal stability may indicate benign background activities, such as regular administrative tasks or scheduled system monitoring. However, it cannot be excluded that persistent activity may also be the result of low-profile malicious campaigns aiming to remain undetected through steady, continuous operations.\n",
        "\n",
        "The plot also reveals periods marked by sudden bursts of activity, where particular fingerprints accumulate a significant number of sessions in a short time. These bursts manifest visually as clusters of large, dark red points and suggest highly concentrated behaviors that could be indicative of automated campaigns or widespread exploitation of vulnerabilities.\n",
        "\n",
        "Another important aspect is the uneven distribution of session counts among fingerprints. Only a small fraction of fingerprints accounts for a large volume of sessions. This skewed pattern is typical of real-world data where a few behaviors, either benign or malicious, dominate due to the popularity of certain attack techniques or the spread of common malware strains.\n",
        "\n",
        "A particularly notable event is the surge in fingerprint diversity and activity around early December 2019. During this period, the number of active fingerprints increases significantly, accompanied by a rise in session counts. This pattern suggests a possible coordinated campaign, where multiple distinct behaviors appear nearly simultaneously, possibly reflecting a shift in tactics or the introduction of new attack vectors.\n",
        "\n",
        "Overall, the scatter plot provides a comprehensive view of the evolution of fingerprint activity over time. It highlights persistent patterns, bursts of concentrated activity, and periods of anomalous behavior, offering a valuable tool for identifying trends that may warrant deeper forensic analysis or targeted threat hunting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xrgh5FasknQx"
      },
      "outputs": [],
      "source": [
        "dates = [item[0][0] for item in sorted_list]\n",
        "fingerprint_indices = [item[0][1] for item in sorted_list]\n",
        "counts = [item[1] for item in sorted_list]\n",
        "\n",
        "log_counts = np.log10(np.array(counts) + 1)\n",
        "sizes = (log_counts) ** 2 * 100\n",
        "red_blue_cmap = LinearSegmentedColormap.from_list('RedBlue', ['#0000FF', '#FF0000'])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(\n",
        "    dates,\n",
        "    fingerprint_indices,\n",
        "    s=sizes,\n",
        "    c=np.array(counts),\n",
        "    cmap=red_blue_cmap,\n",
        "    norm=mcolors.LogNorm(vmin=max(1, min(counts)), vmax=max(counts)),\n",
        "    alpha=0.8,\n",
        "    edgecolors='white',\n",
        "    linewidth=0.5\n",
        ")\n",
        "\n",
        "cbar = plt.colorbar(scatter)\n",
        "cbar.set_label('Number of Sessions', fontsize=18)\n",
        "cbar.ax.tick_params(labelsize=14)\n",
        "\n",
        "plt.xlabel('Date', fontsize=18)\n",
        "plt.ylabel('Fingerprint Index', fontsize=18)\n",
        "plt.title('Sessions Assigned to Fingerprints Over Time', fontsize=18, fontweight='bold', pad=10)\n",
        "\n",
        "plt.xticks(rotation=45, fontsize=14)\n",
        "plt.yticks(fontsize=14)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J481imkRjIUG"
      },
      "source": [
        "### Fingerprint Analysis\n",
        "\n",
        "We analyzed the fingerprints (sequences of MITRE tactic tags) extracted from the dataset of Bash sessions.\n",
        "\n",
        "No fingerprints are present across all sessions. This indicates that there is no unique sequence of tactics common to the entire dataset, reflecting a variety of behaviors and activities.\n",
        "\n",
        "We found 8 fingerprints associated with more than 2000 sessions. These fingerprints likely represent frequent behaviors, which could correspond to either widespread legitimate activities or recurring attack patterns.\n",
        "\n",
        "A few examples of these frequent fingerprints are displayed with their compressed representation (where consecutive identical tactics are merged) and the number of sessions they are linked to.\n",
        "\n",
        "To detect suspicious attack campaigns, we focus on fingerprints with a very high number of associated sessions. The identified fingerprints suggest the presence of large-scale automated activities or repeated attack behaviors, potentially indicating coordinated campaigns.\n",
        "\n",
        "n particular:\n",
        "- Fingerprint **#5** appears less suspicious, consisting mainly of standard Unix commands without clear indicators of malicious behavior.\n",
        "- Fingerprint **#6** shows evidence of SSH key injection via `.ssh/authorized_keys`, likely aiming to establish persistent unauthorized access.\n",
        "- Fingerprints **#1, #2, #3, #4, #7, #8, #9, and #10** include Bash sessions referencing the path `/var/tmp/dota*`.\n",
        "  - This path is highly unusual and not associated with typical system operations.\n",
        "  - Malware often abuses writable directories like `/var/tmp/` to stage files.\n",
        "  - The use of names starting with `dota` does not match standard Linux binaries or files and appears suspicious.\n",
        "  - While references exist in the literature to malicious versions of the Dota game or malware using similar naming, we cannot confirm a direct association.\n",
        "  - Nevertheless, the recurrence of `dota*` patterns, together with removal commands like `rm -rf /var/tmp/dota*`, strongly suggests malicious intent or at least highly suspicious activity.\n",
        "\n",
        "These fingerprints are strong indicators of automated malware campaigns, and particularly consistent with known botnet or cryptomining activities.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEp9xop2jIUG"
      },
      "outputs": [],
      "source": [
        "def compressed_fingerprint(fingerprint):\n",
        "    tags = [id2label[p] for p in fingerprint]\n",
        "    compressed_tags = []\n",
        "    count = 1\n",
        "    for i in range(1, len(tags)):\n",
        "        if tags[i] == tags[i - 1]:\n",
        "            count += 1\n",
        "        else:\n",
        "            compressed_tags.append(f\"{tags[i - 1]}*{count}\" if count > 1 else tags[i - 1])\n",
        "            count = 1\n",
        "    compressed_tags.append(f\"{tags[-1]}*{count}\" if count > 1 else tags[-1])\n",
        "    return compressed_tags\n",
        "\n",
        "fingerprint_counts = Counter(tuple(f) for f in cyberlab_df['fingerprint'])\n",
        "\n",
        "always_present_fingerprints = [f for f, count in fingerprint_counts.items() if count == len(cyberlab_df)]\n",
        "print()\n",
        "print(f\"Fingerprints always present in the dataset: {len(always_present_fingerprints)}\")\n",
        "print(\"- Examples -\")\n",
        "for f in always_present_fingerprints[:5]:\n",
        "    print(f\"• {compressed_fingerprint(f)}\")\n",
        "print()\n",
        "\n",
        "large_fingerprints = [f for f, count in fingerprint_counts.items() if count > 2000]\n",
        "print(f\"Fingerprints with more than 2000 associated sessions: {len(large_fingerprints)}\")\n",
        "print(\"- Examples -\")\n",
        "for f in large_fingerprints:\n",
        "    print(f\"• {compressed_fingerprint(f)}\")\n",
        "print()\n",
        "\n",
        "print(\"Detecting suspicious attack campaigns based on fingerprint activity...\")\n",
        "print(f\"Suspicious fingerprints (more than 2000 sessions): {len(large_fingerprints)}\")\n",
        "print()\n",
        "\n",
        "most_common_fingerprints = fingerprint_counts.most_common(10)\n",
        "\n",
        "print(\"Most common fingerprints:\\n\")\n",
        "for idx, (fingerprint, count) in enumerate(most_common_fingerprints, 1):\n",
        "    compressed = compressed_fingerprint(fingerprint)\n",
        "    sessions = cyberlab_df[cyberlab_df['fingerprint'].apply(lambda x: x == fingerprint)]\n",
        "    session_examples = sessions['trunc_session'].head(3).tolist()\n",
        "\n",
        "    print(f\"Fingerprint #{idx}\")\n",
        "    print(\"-\"*40)\n",
        "    print(f\"Compressed: {compressed}\")\n",
        "    print(f\"Total Sessions: {count}\")\n",
        "    print(f\"Example Sessions:\")\n",
        "    for ex_idx, sess in enumerate(session_examples, 1):\n",
        "        print(f\"  {ex_idx}. {sess}\")\n",
        "    print(\"-\"*40 + \"\\n\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8QaIkXajIUH"
      },
      "outputs": [],
      "source": [
        "\n",
        "legend_order = ['Discovery', 'Persistence', 'Execution', 'Defense Evasion',]\n",
        "legend_order += [l for l in unique_labels if l not in legend_order]\n",
        "\n",
        "legend_handles = [Patch(color=label_colors[label], label=label) for label in legend_order]\n",
        "\n",
        "color_palette = plt.get_cmap('tab10')\n",
        "label_colors = {label: color_palette(i) for i, label in enumerate(unique_labels)}\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 3))\n",
        "\n",
        "for idx, (fingerprint, count) in enumerate(most_common_fingerprints[:10]):\n",
        "    tags = [id2label[p] for p in fingerprint]\n",
        "    colors = [label_colors[tag] for tag in tags]\n",
        "    for i, color in enumerate(colors):\n",
        "        ax.barh(idx, 1, left=i, color=color, edgecolor='none', height=0.8)\n",
        "\n",
        "ax.set_yticks(range(10))\n",
        "ax.set_yticklabels([f\"Fingerprint #{i+1}\" for i in range(10)], fontsize=12)\n",
        "ax.set_xlabel(\"Token Position\", fontsize=14)\n",
        "ax.set_title(\"First 10 Most Common Fingerprints (Tag Sequence Color Bar)\", fontsize=16, fontweight='bold')\n",
        "ax.set_xlim(0, max(len(f) for f, _ in most_common_fingerprints[:10]) + 2)\n",
        "ax.invert_yaxis()\n",
        "ax.tick_params(axis='x', labelsize=12)\n",
        "ax.grid(False)\n",
        "\n",
        "ax.legend(handles=legend_handles, bbox_to_anchor=(1.01, 1), loc='upper left', fontsize=12, title=\"MITRE Tag\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}